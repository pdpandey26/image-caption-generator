{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "169bb99ff8e0709712da41aa546d793361e85b74"
      },
      "cell_type": "code",
      "source": "from numpy import argmax\nfrom pickle import load\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import load_model\nfrom nltk.translate.bleu_score import corpus_bleu",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b2edff231738f98a34931b64306ebd7ea13b1fe8"
      },
      "cell_type": "code",
      "source": "# load doc into memory\ndef load_doc(filename):\n    # open the file as read only\n    file = open(filename, 'r')\n    # read all text\n    text = file.read()\n    # close the file\n    file.close()\n    return text\n\n# load a pre-defined list of photo identifiers\ndef load_set(filename):\n    doc = load_doc(filename)\n    dataset = list()\n    # process line by line\n    for line in doc.split('\\n'):\n        # skip empty lines\n        if len(line) < 1:\n            continue\n        # get the image identifier\n        identifier = line.split('.')[0]\n        dataset.append(identifier)\n    return set(dataset)\n\n# load clean descriptions into memory\ndef load_clean_descriptions(filename, dataset):\n    # load document\n    doc = load_doc(filename)\n    descriptions = dict()\n    for line in doc.split('\\n'):\n        # split line by white space\n        tokens = line.split()\n        # split id from description\n        image_id, image_desc = tokens[0], tokens[1:]\n        # skip images not in the set\n        if image_id in dataset:\n            # create list\n            if image_id not in descriptions:\n                descriptions[image_id] = list()\n            # wrap description in tokens\n            desc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n            # store\n            descriptions[image_id].append(desc)\n    return descriptions\n\n# load photo features\ndef load_photo_features(filename, dataset):\n    # load all features\n    all_features = load(open(filename, 'rb'))\n    # filter features\n    features = {k: all_features[k] for k in dataset}\n    return features\n\n# covert a dictionary of clean descriptions to a list of descriptions\ndef to_lines(descriptions):\n    all_desc = list()\n    for key in descriptions.keys():\n        [all_desc.append(d) for d in descriptions[key]]\n    return all_desc\n\n# fit a tokenizer given caption descriptions\ndef create_tokenizer(descriptions):\n    lines = to_lines(descriptions)\n    tokenizer = Tokenizer()\n    tokenizer.fit_on_texts(lines)\n    return tokenizer\n\n# calculate the length of the description with the most words\ndef max_length(descriptions):\n    lines = to_lines(descriptions)\n    return max(len(d.split()) for d in lines)\n\n# map an integer to a word\ndef word_for_id(integer, tokenizer):\n    for word, index in tokenizer.word_index.items():\n        if index == integer:\n            return word\n    return None\n\n# generate a description for an image\ndef generate_desc(model, tokenizer, photo, max_length):\n    # seed the generation process\n    in_text = 'startseq'\n    # iterate over the whole length of the sequence\n    for i in range(max_length):\n        # integer encode input sequence\n        sequence = tokenizer.texts_to_sequences([in_text])[0]\n        # pad input\n        sequence = pad_sequences([sequence], maxlen=max_length)\n        # predict next word\n        yhat = model.predict([photo,sequence], verbose=0)\n        # convert probability to integer\n        yhat = argmax(yhat)\n        # map integer to word\n        word = word_for_id(yhat, tokenizer)\n        # stop if we cannot map the word\n        if word is None:\n            break\n        # append as input for generating the next word\n        in_text += ' ' + word\n        # stop if we predict the end of the sequence\n        if word == 'endseq':\n            break\n    return in_text\n\n# evaluate the skill of the model\ndef evaluate_model(model, descriptions, photos, tokenizer, max_length):\n    actual, predicted = list(), list()\n    # step over the whole set\n    for key, desc_list in descriptions.items():\n        # generate description\n        yhat = generate_desc(model, tokenizer, photos[key], max_length)\n        # store actual and predicted\n        references = [d.split() for d in desc_list]\n        actual.append(references)\n        predicted.append(yhat.split())\n    # calculate BLEU score\n    print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n    print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n    print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n    print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# prepare tokenizer on train set\n\n# load training dataset (6K)\nfilename = '../input/8kflickrfeature/flickr8k_text/Flickr_8k.trainImages.txt'\ntrain = load_set(filename)\nprint('Dataset: %d' % len(train))\n# descriptions\ntrain_descriptions = load_clean_descriptions('../input/text-data-exploxe/descriptions.txt', train)\nprint('Descriptions: train=%d' % len(train_descriptions))\n# prepare tokenizer\ntokenizer = create_tokenizer(train_descriptions)\nvocab_size = len(tokenizer.word_index) + 1\nprint('Vocabulary Size: %d' % vocab_size)\n# determine the maximum sequence length\nmax_length = max_length(train_descriptions)\nprint('Description Length: %d' % max_length)\n\n# prepare test set\n\n# load test set\nfilename = '../input/8kflickrfeature/flickr8k_text/Flickr_8k.testImages.txt'\ntest = load_set(filename)\nprint('Dataset: %d' % len(test))\n# descriptions\ntest_descriptions = load_clean_descriptions('../input/text-data-exploxe/descriptions.txt', test)\nprint('Descriptions: test=%d' % len(test_descriptions))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "de0414e24bf377942d546ad91faade9c73ec49aa"
      },
      "cell_type": "code",
      "source": "# photo features\ntest_features = load_photo_features('../input/flirck-8k-dataset-explore-image/features.pkl', test)\nprint('VGG\\nPhotos: test=%d' % len(test_features))\n\n# load the model\nfilename = '../input/development-model/model_18.h5'\nmodel = load_model(filename)\n# evaluate model\nevaluate_model(model, test_descriptions, test_features, tokenizer, max_length)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "55972cc176df04788c7ac0229e67f3aeafa6a5b3"
      },
      "cell_type": "code",
      "source": "# photo features\ntest_features = load_photo_features('../input/flirck-8k-dataset-explore-image-resnet/features.pkl', test)\nprint('Resnet\\nPhotos: test=%d' % len(test_features))\n\n# load the model\nfilename = '../input/development-model-resnet50/model_19.h5'\nmodel = load_model(filename)\n# evaluate model\nevaluate_model(model, test_descriptions, test_features, tokenizer, max_length)",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}