{
  "cells": [
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "from keras.preprocessing.text import Tokenizer\nfrom pickle import dump\n\n# load doc into memory\ndef load_doc(filename):\n    # open the file as read only\n    file = open(filename, 'r')\n    # read all text\n    text = file.read()\n    # close the file\n    file.close()\n    return text\n\n# load a pre-defined list of photo identifiers\ndef load_set(filename):\n    doc = load_doc(filename)\n    dataset = list()\n    # process line by line\n    for line in doc.split('\\n'):\n        # skip empty lines\n        if len(line) < 1:\n            continue\n        # get the image identifier\n        identifier = line.split('.')[0]\n        dataset.append(identifier)\n    return set(dataset)\n\n# load clean descriptions into memory\ndef load_clean_descriptions(filename, dataset):\n    # load document\n    doc = load_doc(filename)\n    descriptions = dict()\n    for line in doc.split('\\n'):\n        # split line by white space\n        tokens = line.split()\n        # split id from description\n        image_id, image_desc = tokens[0], tokens[1:]\n        # skip images not in the set\n        if image_id in dataset:\n            # create list\n            if image_id not in descriptions:\n                descriptions[image_id] = list()\n            # wrap description in tokens\n            desc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n            # store\n            descriptions[image_id].append(desc)\n    return descriptions\n\n# covert a dictionary of clean descriptions to a list of descriptions\ndef to_lines(descriptions):\n    all_desc = list()\n    for key in descriptions.keys():\n        [all_desc.append(d) for d in descriptions[key]]\n    return all_desc\n\n# fit a tokenizer given caption descriptions\ndef create_tokenizer(descriptions):\n    lines = to_lines(descriptions)\n    tokenizer = Tokenizer()\n    tokenizer.fit_on_texts(lines)\n    return tokenizer\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a3c760837f8b1e00badb740d7682a6c4a40893e4"
      },
      "cell_type": "code",
      "source": "# load training dataset (6K)\nfilename = '../input/8kflickrfeature/flickr8k_text/Flickr_8k.trainImages.txt'\ntrain = load_set(filename)\nprint('Dataset: %d' % len(train))\n# descriptions\ntrain_descriptions = load_clean_descriptions('../input/text-data-exploxe/descriptions.txt', train)\nprint('Descriptions: train=%d' % len(train_descriptions))\n# prepare tokenizer\ntokenizer = create_tokenizer(train_descriptions)\n# save the tokenizer\ndump(tokenizer, open('tokenizer.pkl', 'wb'))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7dcaf7cabfd4540443113e9af19f23f45dc7335b"
      },
      "cell_type": "code",
      "source": "from pickle import load\nfrom numpy import argmax\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.applications.resnet50 import ResNet50\nfrom keras.preprocessing.image import load_img\nfrom keras.preprocessing.image import img_to_array\nfrom keras.applications.resnet50 import preprocess_input\nfrom keras.models import Model\nfrom keras.models import load_model",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4b98d66eab8342a04987e31c77bbe5412f70203d"
      },
      "cell_type": "code",
      "source": "# extract features from each photo in the directory\ndef extract_features(filename):\n    # load the model\n    model = ResNet50()\n    # re-structure the model\n    model.layers.pop()\n    model = Model(inputs=model.inputs, outputs=model.layers[-1].output)\n    # load the photo\n    image = load_img(filename, target_size=(224, 224))\n    # convert the image pixels to a numpy array\n    image = img_to_array(image)\n    # reshape data for the model\n    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n    # prepare the image for the Resnet50 model\n    image = preprocess_input(image)\n    # get features\n    feature = model.predict(image, verbose=0)\n    return feature\n\n# map an integer to a word\ndef word_for_id(integer, tokenizer):\n    for word, index in tokenizer.word_index.items():\n        if index == integer:\n            return word\n    return None\n\n# generate a description for an image\ndef generate_desc(model, tokenizer, photo, max_length):\n    # seed the generation process\n    in_text = 'startseq'\n    # iterate over the whole length of the sequence\n    for i in range(max_length):\n        # integer encode input sequence\n        sequence = tokenizer.texts_to_sequences([in_text])[0]\n        # pad input\n        sequence = pad_sequences([sequence], maxlen=max_length)\n        # predict next word\n        yhat = model.predict([photo,sequence], verbose=0)\n        # convert probability to integer\n        yhat = argmax(yhat)\n        # map integer to word\n        word = word_for_id(yhat, tokenizer)\n        # stop if we cannot map the word\n        if word is None:\n            break\n        # append as input for generating the next word\n        in_text += ' ' + word\n        # stop if we predict the end of the sequence\n        if word == 'endseq':\n            break\n    return in_text\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5c11e70552fd421be228fb3ecdbb5618a9c739b3"
      },
      "cell_type": "code",
      "source": "# load the tokenizer\ntokenizer = load(open('tokenizer.pkl', 'rb'))\n# pre-define the max sequence length (from training)\nmax_length = 34\n# load the model\nmodel = load_model('../input/development-model-resnet50/model_19.h5')\n# load and prepare the photograph\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "be378e2438249a9e25030c6b9dcf2cf0ce270a42"
      },
      "cell_type": "code",
      "source": "from keras.preprocessing.image import load_img\nfrom matplotlib.pyplot import imshow\nimport numpy as np\nimport os",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "24a534985c3dd25e2133c1bc74a02bcae9565747"
      },
      "cell_type": "code",
      "source": "import matplotlib.pyplot as plt\nimport numpy as np\n\ndef show_images(images, cols = 1, titles = None):\n    \"\"\"Display a list of images in a single figure with matplotlib.\n    \n    Parameters\n    ---------\n    images: List of np.arrays compatible with plt.imshow.\n    \n    cols (Default = 1): Number of columns in figure (number of rows is \n                        set to np.ceil(n_images/float(cols))).\n    \n    titles: List of titles corresponding to each image. Must have\n            the same length as titles.\n    \"\"\"\n    assert((titles is None)or (len(images) == len(titles)))\n    n_images = len(images)\n    if titles is None: titles = ['Image (%d)' % i for i in range(1,n_images + 1)]\n    fig = plt.figure()\n    for n, (image, title) in enumerate(zip(images, titles)):\n        a = fig.add_subplot(cols, np.ceil(n_images/float(cols)), n + 1)\n        if image.ndim == 2:\n            plt.gray()\n        plt.imshow(image)\n        a.set_title(title)\n    fig.set_size_inches(np.array(fig.get_size_inches()) * n_images)\n    plt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3913f60dbac637e036f399db80c98d2403cd2199"
      },
      "cell_type": "code",
      "source": "path, dirs, imgs = next(os.walk(\"../input/flickr-image-dataset/flickr30k_images/flickr30k_images/flickr30k_images/\"))\nimg_path = '../input/flickr-image-dataset/flickr30k_images/flickr30k_images/flickr30k_images/'\nlist_img = []\nlist_desc = []\nfor i in range(100,140):\n    image = load_img(img_path + imgs[i])\n    list_img.append(np.asarray(image))\n    # generate description\n    photo = extract_features(img_path + imgs[i])\n    description = generate_desc(model, tokenizer, photo, max_length)\n    list_desc.append(description)\n    if(len(list_img) == 5):\n        show_images(images=list_img, cols=len(list_img), titles=list_desc)\n        list_img = []\n        list_desc = []\n    \n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "91dbc0e6018f00a780c0148feb8aace6d176a36d"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}